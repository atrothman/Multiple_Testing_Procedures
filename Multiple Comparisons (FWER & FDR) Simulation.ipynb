{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## import libraries ##\n",
    "######################\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import time, sys\n",
    "import numpy as np\n",
    "np.random.seed(1081565)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## print progress bar helper function ##\n",
    "########################################\n",
    "def printProgressBar(i,max,postText):\n",
    "    n_bar =5 #size of progress bar\n",
    "    j= i/max\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(f\"[{'=' * int(n_bar * j):{n_bar}s}] {int(100 * j)}%,  {postText}\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## create simulated dataset ##\n",
    "##############################\n",
    "def simulate_dataset(n=100, correlated='none', all_null=True, permutation=False, seed=123):\n",
    "    np.random.seed(seed)\n",
    "    if(correlated=='none'):\n",
    "        if(all_null):       \n",
    "            mu = np.array(n*[0])\n",
    "            df = pd.DataFrame()\n",
    "            df['ID'] = list(range(1,mu.shape[0]+1))\n",
    "            df['true_effect'] = mu\n",
    "        else:\n",
    "            mu = np.array(int(n/4)*[-5,0,0,0])\n",
    "            df = pd.DataFrame()\n",
    "            df['ID'] = list(range(1,mu.shape[0]+1))  \n",
    "            df['true_effect'] = mu\n",
    "        cov_matrix = np.eye(mu.shape[0], k=0)\n",
    "        df['measured_effect'] = np.random.multivariate_normal(mean=mu, cov=cov_matrix, size=1)[0]\n",
    "        if(permutation):\n",
    "            if(all_null==False):\n",
    "                mu_corrected = mu.copy()\n",
    "                cov_matrix_corrected = cov_matrix.copy()\n",
    "                for i in range(mu.shape[0]-1, -1, -1):\n",
    "                    if(mu[i]!=0):\n",
    "                        mu_corrected = np.delete(mu_corrected, i)\n",
    "                        cov_matrix_corrected = np.delete(cov_matrix_corrected, i, 0)\n",
    "                        cov_matrix_corrected = np.delete(cov_matrix_corrected, i, 1)\n",
    "                perm_matrix = abs(np.random.multivariate_normal(mean=mu_corrected, cov=cov_matrix_corrected, size=10000))\n",
    "            else:\n",
    "                perm_matrix = abs(np.random.multivariate_normal(mean=mu, cov=cov_matrix, size=10000))\n",
    "            df_perm = pd.DataFrame()\n",
    "            df_perm['test_stat'] = perm_matrix.max(axis=1)\n",
    "            del perm_matrix\n",
    "            \n",
    "        del mu, cov_matrix\n",
    "    \n",
    "    if(correlated=='positive'):\n",
    "        if(all_null):       \n",
    "            mu = np.array(n*[0])\n",
    "            df = pd.DataFrame()\n",
    "            df['ID'] = list(range(1,mu.shape[0]+1))\n",
    "            df['true_effect'] = mu\n",
    "        else:\n",
    "            mu = np.array(int(n/4)*[-5,0,0,0])\n",
    "            df = pd.DataFrame()\n",
    "            df['ID'] = list(range(1,mu.shape[0]+1))  \n",
    "            df['true_effect'] = mu\n",
    "        m=int(mu.shape[0]/1.15)\n",
    "        phi = 0.95\n",
    "        phi_matrix = (phi*np.eye(m, k=1)) + (phi*np.eye(m, k=-1))\n",
    "        cov_matrix = np.eye(mu.shape[0], k=0)\n",
    "        cov_matrix[0:phi_matrix.shape[0], 0:phi_matrix.shape[1]] = cov_matrix[0:phi_matrix.shape[0], 0:phi_matrix.shape[1]] + phi_matrix\n",
    "        for i in range(2,cov_matrix.shape[0],2):\n",
    "            cov_matrix[i,i-1]=0\n",
    "            cov_matrix[i-1,i]=0\n",
    "        df['measured_effect'] = np.random.multivariate_normal(mean=mu, cov=cov_matrix, size=1)[0]\n",
    "        \n",
    "        if(permutation):\n",
    "            if(all_null==False):\n",
    "                mu_corrected = mu.copy()\n",
    "                cov_matrix_corrected = cov_matrix.copy()\n",
    "                for i in range(mu.shape[0]-1, -1, -1):\n",
    "                    if(mu[i]!=0):\n",
    "                        mu_corrected = np.delete(mu_corrected, i)\n",
    "                        cov_matrix_corrected = np.delete(cov_matrix_corrected, i, 0)\n",
    "                        cov_matrix_corrected = np.delete(cov_matrix_corrected, i, 1)\n",
    "                perm_matrix = abs(np.random.multivariate_normal(mean=mu_corrected, cov=cov_matrix_corrected, size=10000))\n",
    "            else:\n",
    "                perm_matrix = abs(np.random.multivariate_normal(mean=mu, cov=cov_matrix, size=10000))\n",
    "            df_perm = pd.DataFrame()\n",
    "            df_perm['test_stat'] = perm_matrix.max(axis=1)\n",
    "            del perm_matrix\n",
    "        \n",
    "        del mu, m, cov_matrix, phi, phi_matrix\n",
    "\n",
    "    if(permutation):\n",
    "        return(df, df_perm)\n",
    "    else:\n",
    "        return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "## Family Wise Error Rate (FWER) Methods ##\n",
    "###########################################\n",
    "def FWER_adjustment(FWER_alpha=0.05, method='Bonferoni', all_null=True, correlated='none', trials=1000, n=100):\n",
    "    \n",
    "    n = int(n/4)*4\n",
    "    ## initialize dataframe to hold final results \n",
    "    df_results = pd.DataFrame()\n",
    "    df_results['trial'] = list(range(0,trials))\n",
    "    df_results['sig_p_value'] = None\n",
    "    \n",
    "    ## specify individual level alpha based on FWER contol method\n",
    "    if(method=='Bonferoni'):   \n",
    "        alpha = FWER_alpha/n\n",
    "    if(method=='Sidak'):   \n",
    "        alpha = 1 - ((1-FWER_alpha)**(1/n))\n",
    "    if((method=='Holm–Bonferroni') or (method=='Hochberg')):\n",
    "        alpha = [(FWER_alpha / x) for x in list(range(n, 0, -1))]\n",
    "    if(method=='Holm–Sidak'):\n",
    "        alpha = [(1 - ((1-FWER_alpha)**(1/x))) for x in list(range(n, 0, -1))]\n",
    "        \n",
    "    ## conduct simulations, and print results\n",
    "    for i in range(0, trials):\n",
    "        printProgressBar(i+1, trials, \"\")\n",
    "        time.sleep(0.1) \n",
    "        \n",
    "        if(method=='Permutation'):\n",
    "            df, df_perm = simulate_dataset(n=n, correlated=correlated, all_null=all_null, permutation=True, seed=i)\n",
    "            df = df.loc[df['true_effect']==0,:].reset_index(drop=True)\n",
    "            test_stat = max(abs(df['measured_effect']))\n",
    "            p_value = df_perm.loc[df_perm['test_stat']>=test_stat].shape[0] / df_perm.shape[0]\n",
    "            if(p_value <= FWER_alpha):\n",
    "                df_results.loc[i, 'sig_p_value'] = 1\n",
    "            else:\n",
    "                df_results.loc[i, 'sig_p_value'] = 0\n",
    "            del df, df_perm, p_value\n",
    "        else:\n",
    "            df = simulate_dataset(n=n, correlated=correlated, all_null=all_null, permutation=False, seed=i)\n",
    "            df['cdf'] = stats.norm.cdf(df['measured_effect'], loc=0, scale=1)\n",
    "            df['1-cdf'] = 1-df['cdf']\n",
    "            df['p-value'] = 2*df[['cdf', '1-cdf']].min(axis=1)\n",
    "            df = df.sort_values(by=['p-value'], ascending=True).reset_index(drop=True)\n",
    "            df['alpha'] = alpha\n",
    "            if(method=='Hochberg'):\n",
    "                df['alpha_p-value_difference'] = df['alpha']-df['p-value']\n",
    "                df = df.loc[df['alpha_p-value_difference']>=0, :].reset_index(drop=True)\n",
    "                df = df.sort_values(by=['alpha_p-value_difference'], ascending=True).reset_index(drop=True)                       \n",
    "            df = df.loc[df['true_effect']==0,:].reset_index(drop=True)\n",
    "            if((df.shape[0]>0) and (df.loc[0, 'p-value'] <= df.loc[0, 'alpha'])):\n",
    "                df_results.loc[i, 'sig_p_value'] = 1\n",
    "            else:\n",
    "                df_results.loc[i, 'sig_p_value'] = 0\n",
    "            del df\n",
    "    \n",
    "    mean = round(df_results['sig_p_value'].sum() / df_results.shape[0], 3)\n",
    "    sd = np.sqrt((mean*(1-mean))/trials)\n",
    "    l_bound = round(mean-(1.96*sd), 3)\n",
    "    u_bound = round(mean+(1.96*sd), 3)\n",
    "    \n",
    "    if((all_null==True) and  correlated=='none'):\n",
    "        print('FWER via ' + method + ' - all hypotheses null with no correlation: ' + str(mean) + ' , 95% CI (' + str(l_bound) + ' , ' + str(u_bound) + ')')\n",
    "    if((all_null==True) and  correlated=='positive'):\n",
    "        print('FWER via ' + method + ' - all hypotheses null with positive correlation: ' + str(mean) + ' , 95% CI (' + str(l_bound) + ' , ' + str(u_bound) + ')')\n",
    "    if((all_null==False) and  correlated=='none'):\n",
    "        print('FWER via ' + method + ' - 75% hypotheses null with no correlation: ' + str(mean) + ' , 95% CI (' + str(l_bound) + ' , ' + str(u_bound) + ')')\n",
    "    if((all_null==False) and  correlated=='positive'):\n",
    "        print('FWER via ' + method + ' - 75% hypotheses null with positive correlation: ' + str(mean) + ' , 95% CI (' + str(l_bound) + ' , ' + str(u_bound) + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## False Discovery Rate (FDR) Methods ##\n",
    "########################################\n",
    "def FDR_adjustment(FDR_alpha=0.05, method='Benjamini–Hochberg', all_null=True, correlated='none', trials=1000, n=1000):\n",
    "    \n",
    "    n = int(n/4)*4\n",
    "    ## initialize dataframe to hold final results \n",
    "    df_results = pd.DataFrame()\n",
    "    df_results['trial'] = list(range(0,trials))\n",
    "    df_results['sig_p_value'] = None\n",
    "    \n",
    "    ## specify individual level alpha based on FDR contol method\n",
    "    if(method=='Benjamini–Hochberg'):\n",
    "        alpha = [((FDR_alpha*x) / n) for x in list(range(1, n+1))]\n",
    "    if(method=='Benjamini–Yekutieli'):\n",
    "        C = sum([(1/x) for x in list(range(1, n+1))])\n",
    "        alpha = [((FDR_alpha*x) / (n*C)) for x in list(range(1, n+1))]\n",
    "        \n",
    "    for i in range(0, trials):\n",
    "        printProgressBar(i+1, trials, \"\")\n",
    "        time.sleep(0.1)     \n",
    "        \n",
    "        df = simulate_dataset(n=n, correlated=correlated, all_null=all_null, permutation=False, seed=i)\n",
    "        df['cdf'] = stats.norm.cdf(df['measured_effect'], loc=0, scale=1)\n",
    "        df['1-cdf'] = 1-df['cdf']\n",
    "        df['p-value'] = 2*df[['cdf', '1-cdf']].min(axis=1)\n",
    "        df = df.sort_values(by=['p-value'], ascending=True).reset_index(drop=True)\n",
    "        df['alpha'] = alpha\n",
    "        df['alpha_p-value_difference'] = df['alpha']-df['p-value']\n",
    "        df['rank_order'] = list(range(0,df.shape[0]))\n",
    "        max_rank = df.loc[df['alpha_p-value_difference']>=0, 'rank_order'].max()\n",
    "        df = df.loc[df['rank_order']<=max_rank, :].reset_index(drop=True)\n",
    "        if(df.shape[0]==0):\n",
    "            df_results.loc[i, 'sig_p_value'] = 0\n",
    "        else:\n",
    "            df_results.loc[i, 'sig_p_value'] = df.loc[df['true_effect']==0, :].shape[0] / df.shape[0]\n",
    "        del df, max_rank\n",
    "        \n",
    "    mean = round(df_results['sig_p_value'].sum() / df_results.shape[0], 3)\n",
    "    sd = sd = np.sqrt(np.var(df_results['sig_p_value']) / trials)\n",
    "    l_bound = round(mean-(1.96*sd), 3)\n",
    "    u_bound = round(mean+(1.96*sd), 3)\n",
    "    \n",
    "    if((all_null==True) and  correlated=='none'):\n",
    "        print('FDR via ' + method + ' - all hypotheses null with no correlation: ' + str(mean) + ' , 95% CI (' + str(l_bound) + ' , ' + str(u_bound) + ')')\n",
    "    if((all_null==True) and  correlated=='positive'):\n",
    "        print('FDR via ' + method + ' - all hypotheses null with positive correlation: ' + str(mean) + ' , 95% CI (' + str(l_bound) + ' , ' + str(u_bound) + ')')\n",
    "    if((all_null==False) and  correlated=='none'):\n",
    "        print('FDR via ' + method + ' - 75% hypotheses null with no correlation: ' + str(mean) + ' , 95% CI (' + str(l_bound) + ' , ' + str(u_bound) + ')')\n",
    "    if((all_null==False) and  correlated=='positive'):\n",
    "        print('FDR via ' + method + ' - 75% hypotheses null with positive correlation: ' + str(mean) + ' , 95% CI (' + str(l_bound) + ' , ' + str(u_bound) + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "## FWER Methods ##\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====] 100%,  FWER via Bonferoni - all hypotheses null with no correlation: 0.045 , 95% CI (0.036 , 0.054)\n",
      "[=====] 100%,  FWER via Bonferoni - all hypotheses null with positive correlation: 0.038 , 95% CI (0.03 , 0.046)\n",
      "[=====] 100%,  FWER via Bonferoni - 75% hypotheses null with no correlation: 0.036 , 95% CI (0.028 , 0.044)\n",
      "[=====] 100%,  FWER via Bonferoni - 75% hypotheses null with positive correlation: 0.029 , 95% CI (0.022 , 0.036)\n"
     ]
    }
   ],
   "source": [
    "## Bonferoni:\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Bonferoni', all_null=True, correlated='none', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Bonferoni', all_null=True, correlated='positive', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Bonferoni', all_null=False, correlated='none', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Bonferoni', all_null=False, correlated='positive', trials=2000, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====] 100%,  FWER via Sidak - all hypotheses null with no correlation: 0.046 , 95% CI (0.037 , 0.055)\n",
      "[=====] 100%,  FWER via Sidak - all hypotheses null with positive correlation: 0.039 , 95% CI (0.031 , 0.047)\n",
      "[=====] 100%,  FWER via Sidak - 75% hypotheses null with no correlation: 0.038 , 95% CI (0.03 , 0.046)\n",
      "[=====] 100%,  FWER via Sidak - 75% hypotheses null with positive correlation: 0.03 , 95% CI (0.023 , 0.037)\n"
     ]
    }
   ],
   "source": [
    "## Sidak:\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Sidak', all_null=True, correlated='none', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Sidak', all_null=True, correlated='positive', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Sidak', all_null=False, correlated='none', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Sidak', all_null=False, correlated='positive', trials=2000, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====] 100%,  FWER via Holm–Bonferroni - all hypotheses null with no correlation: 0.045 , 95% CI (0.036 , 0.054)\n",
      "[=====] 100%,  FWER via Holm–Bonferroni - all hypotheses null with positive correlation: 0.038 , 95% CI (0.03 , 0.046)\n",
      "[=====] 100%,  FWER via Holm–Bonferroni - 75% hypotheses null with no correlation: 0.05 , 95% CI (0.04 , 0.06)\n",
      "[=====] 100%,  FWER via Holm–Bonferroni - 75% hypotheses null with positive correlation: 0.036 , 95% CI (0.028 , 0.044)\n"
     ]
    }
   ],
   "source": [
    "## Holm–Bonferroni:\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Holm–Bonferroni', all_null=True, correlated='none', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Holm–Bonferroni', all_null=True, correlated='positive', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Holm–Bonferroni', all_null=False, correlated='none', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Holm–Bonferroni', all_null=False, correlated='positive', trials=2000, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====] 100%,  FWER via Holm–Sidak - all hypotheses null with no correlation: 0.046 , 95% CI (0.037 , 0.055)\n",
      "[=====] 100%,  FWER via Holm–Sidak - all hypotheses null with positive correlation: 0.039 , 95% CI (0.031 , 0.047)\n",
      "[=====] 100%,  FWER via Holm–Sidak - 75% hypotheses null with no correlation: 0.051 , 95% CI (0.041 , 0.061)\n",
      "[=====] 100%,  FWER via Holm–Sidak - 75% hypotheses null with positive correlation: 0.038 , 95% CI (0.03 , 0.046)\n"
     ]
    }
   ],
   "source": [
    "## Holm–Sidak:\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Holm–Sidak', all_null=True, correlated='none', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Holm–Sidak', all_null=True, correlated='positive', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Holm–Sidak', all_null=False, correlated='none', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Holm–Sidak', all_null=False, correlated='positive', trials=2000, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====] 100%,  FWER via Hochberg - all hypotheses null with no correlation: 0.045 , 95% CI (0.036 , 0.054)\n",
      "[=====] 100%,  FWER via Hochberg - all hypotheses null with positive correlation: 0.038 , 95% CI (0.03 , 0.046)\n",
      "[=====] 100%,  FWER via Hochberg - 75% hypotheses null with no correlation: 0.05 , 95% CI (0.04 , 0.06)\n",
      "[=====] 100%,  FWER via Hochberg - 75% hypotheses null with positive correlation: 0.036 , 95% CI (0.028 , 0.044)\n"
     ]
    }
   ],
   "source": [
    "## Hochberg:\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Hochberg', all_null=True, correlated='none', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Hochberg', all_null=True, correlated='positive', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Hochberg', all_null=False, correlated='none', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Hochberg', all_null=False, correlated='positive', trials=2000, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====] 100%,  FWER via Permutation - all hypotheses null with no correlation: 0.046 , 95% CI (0.037 , 0.055)\n",
      "[=====] 100%,  FWER via Permutation - all hypotheses null with positive correlation: 0.049 , 95% CI (0.04 , 0.058)\n",
      "[=====] 100%,  FWER via Permutation - 75% hypotheses null with no correlation: 0.051 , 95% CI (0.041 , 0.061)\n",
      "[=====] 100%,  FWER via Permutation - 75% hypotheses null with positive correlation: 0.052 , 95% CI (0.042 , 0.062)\n"
     ]
    }
   ],
   "source": [
    "## Permutation:\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Permutation', all_null=True, correlated='none', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Permutation', all_null=True, correlated='positive', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Permutation', all_null=False, correlated='none', trials=2000, n=200)\n",
    "FWER_adjustment(FWER_alpha=0.05, method='Permutation', all_null=False, correlated='positive', trials=2000, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "## FDR Methods ##\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====] 100%,  FDR via Benjamini–Hochberg - all hypotheses null with no correlation: 0.046 , 95% CI (0.037 , 0.055)\n",
      "[=====] 100%,  FDR via Benjamini–Hochberg - all hypotheses null with positive correlation: 0.041 , 95% CI (0.032 , 0.05)\n",
      "[=====] 100%,  FDR via Benjamini–Hochberg - 75% hypotheses null with no correlation: 0.038 , 95% CI (0.037 , 0.039)\n",
      "[=====] 100%,  FDR via Benjamini–Hochberg - 75% hypotheses null with positive correlation: 0.038 , 95% CI (0.037 , 0.039)\n"
     ]
    }
   ],
   "source": [
    "## Benjamini–Hochberg:\n",
    "FDR_adjustment(FDR_alpha=0.05, method='Benjamini–Hochberg',all_null=True, correlated='none', trials=2000, n=200)\n",
    "FDR_adjustment(FDR_alpha=0.05, method='Benjamini–Hochberg',all_null=True, correlated='positive', trials=2000, n=200)\n",
    "FDR_adjustment(FDR_alpha=0.05, method='Benjamini–Hochberg',all_null=False, correlated='none', trials=2000, n=200)\n",
    "FDR_adjustment(FDR_alpha=0.05, method='Benjamini–Hochberg',all_null=False, correlated='positive', trials=2000, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====] 100%,  FDR via Benjamini–Yekutieli - all hypotheses null with no correlation: 0.011 , 95% CI (0.006 , 0.016)\n",
      "[=====] 100%,  FDR via Benjamini–Yekutieli - all hypotheses null with positive correlation: 0.005 , 95% CI (0.002 , 0.008)\n",
      "[=====] 100%,  FDR via Benjamini–Yekutieli - 75% hypotheses null with no correlation: 0.007 , 95% CI (0.007 , 0.007)\n",
      "[=====] 100%,  FDR via Benjamini–Yekutieli - 75% hypotheses null with positive correlation: 0.007 , 95% CI (0.006 , 0.008)\n"
     ]
    }
   ],
   "source": [
    "## Benjamini–Yekutieli:\n",
    "FDR_adjustment(FDR_alpha=0.05, method='Benjamini–Yekutieli',all_null=True, correlated='none', trials=2000, n=200)\n",
    "FDR_adjustment(FDR_alpha=0.05, method='Benjamini–Yekutieli',all_null=True, correlated='positive', trials=2000, n=200)\n",
    "FDR_adjustment(FDR_alpha=0.05, method='Benjamini–Yekutieli',all_null=False, correlated='none', trials=2000, n=200)\n",
    "FDR_adjustment(FDR_alpha=0.05, method='Benjamini–Yekutieli',all_null=False, correlated='positive', trials=2000, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
